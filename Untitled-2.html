<table><tr><th>id</th><th>title</th><th>authors</th><th>year</th><th>thumb</th><th>paper_type</th><th>code</th><th>pdf</th><th>pub</th><th>project</th><tr><tr><td>2</td><td>Composing Semantic Collage for Image Retargeting</td><td>Si Liu, Zhen Wei, Yao Sun, Xinyu Ou, Junyu Lin, Bin Liu, Ming-Hsuan Yang</td><td>2018</td><td>paper_thumb&#x2F;WeChat_Screenshot_20190321133915.png</td><td>journal</td><td></td><td>paper&#x2F;tip18_retarget_3.31.pdf</td><td>TIP 2018</td><td></td></tr><tr><td>3</td><td>Correlation Particle Filter for Visual Tracking</td><td>Tianzhu Zhang, Si Liu, Changsheng Xu, Bin Liu, Ming-Hsuan Yang</td><td>2018</td><td>paper_thumb&#x2F;WeChat_Screenshot_20190321134048.png</td><td>journal</td><td></td><td>paper&#x2F;Correlation_Particle_Filter_for_Visual_Tracking.pdf</td><td>TIP 2018</td><td></td></tr><tr><td>5</td><td>Robust Target Tracking by Online Random Forests and Superpixels</td><td>Wei Wang, Chunping Wang, Si Liu, Xiaochun Cao</td><td>2018</td><td>paper_thumb&#x2F;WeChat_Screenshot_20190321134301.png</td><td>journal</td><td></td><td>paper&#x2F;Robust-tcsvt-2016.pdf</td><td>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) 2018</td><td></td></tr><tr><td>6</td><td>Cross-domain Human Parsing via Adversarial Feature and Label Adaptation</td><td>Si Liu, Yao Sun, Defa Zhu, Guanghui Ren, Yu Chen, Jiashi Feng, and Jizhong Han</td><td>2018</td><td>paper_thumb&#x2F;WeChat_Screenshot_20190321134328.png</td><td>conference</td><td></td><td>paper&#x2F;17058-72879-2-SM.pdf</td><td>AAAI 2018</td><td></td></tr><tr><td>7</td><td>Learning Adaptive Receptive Fields for Deep Image Parsing Network</td><td>Zhen Wei, Yao Sun, Junyu Lin, Si Liu</td><td>2018</td><td>paper_thumb&#x2F;WeChat_Screenshot_20190321134427.png</td><td>conference</td><td></td><td>paper&#x2F;CVM_camera-ready.pdf</td><td>Computational Visual Media 2018</td><td></td></tr><tr><td>8</td><td>Ensemble Soft-Margin Softmax Loss for Image Classification</td><td>Xiaobo Wang, Shifeng Zhang, Zhen Lei, Si Liu, Xiaojie Guo, Stan Z. Li</td><td>2018</td><td>paper_thumb&#x2F;WeChat_Screenshot_20190321134529.png</td><td>conference</td><td></td><td>paper&#x2F;ijcai_2018.pdf</td><td>IJCAI 2018</td><td></td></tr><tr><td>9</td><td>BeautyGAN: Instance-level Facial Makeup Transfer with Deep Generative Adversarial Network</td><td>Tingting Li, Ruihe Qian, Chao Dong, Si Liu, Qiong Yan, Wenwu Zhu, Liang Lin</td><td>2018</td><td>paper_thumb&#x2F;WeChat_Screenshot_20190321134609.png</td><td>conference</td><td>https:&#x2F;&#x2F;github.com&#x2F;wtjiang98&#x2F;BeautyGAN_pytorch</td><td>paper&#x2F;BeautyGAN-camera-ready.pdf</td><td>ACM MM 2018</td><td>http:&#x2F;&#x2F;colalab.org&#x2F;projects&#x2F;BeautyGAN</td></tr><tr><td>10</td><td>Improved Search in Hamming Space using Deep Multi-Index Hashing</td><td>Hanjiang Lai, Yan Pan, Si Liu, Zhenbin Weng, Jian Yin</td><td>2018</td><td>paper_thumb&#x2F;WeChat_Screenshot_20190321134640.png</td><td>journal</td><td></td><td>paper&#x2F;bare_jrnl.pdf</td><td>TCSVT 2018</td><td></td></tr><tr><td>11</td><td>Magic-wall: Visualizing Room Decoration by Enhanced Wall Segmentation</td><td>Ting Liu, Yunchao Wei, Yao Zhao, Si Liu, Shikui Wei</td><td>2019</td><td>paper_thumb&#x2F;20190401203110.png</td><td>journal</td><td>#</td><td>paper&#x2F;bare_jrnl_JfqwY5c.pdf</td><td>TIP 2019</td><td>#</td></tr><tr><td>13</td><td>Building Detail-Sensitive Semantic Segmentation Networks with Polynomial Pooling</td><td>Zhen Wei, Jingyi Zhang, Fumin Shen, Li Liu, Fan Zhu, Yi Zhou, Si Liu, Yao Sun, Ling Shao</td><td>2019</td><td>paper_thumb&#x2F;Screenshot_from_2019-04-05_16-19-11.png</td><td>conference</td><td></td><td>paper&#x2F;paper_final.pdf</td><td>CVPR 2019</td><td></td></tr><tr><td>14</td><td>Accurate Facial Image Parsing at Real-Time Speed</td><td>Zhen Wei,  Si Liu, Yao Sun, Hefei Ling</td><td>2019</td><td>paper_thumb&#x2F;Screenshot_from_2019-04-05_16-19-31.png</td><td>journal</td><td></td><td>paper&#x2F;bare_jrnl_EjnyBQb.pdf</td><td>TIP 2019</td><td></td></tr><tr><td>15</td><td>RotateView: A Video Composition System for Interactive Product Display</td><td>Shan An, Si Liu*, Zhibiao Huang, Guangfu Che, Qian Bao, Zhaoqi Zhu, Yu Chen, Dennis Z. Weng</td><td>2019</td><td>paper_thumb&#x2F;QQ20190528194546.png</td><td>journal</td><td></td><td>paper&#x2F;RotateView.pdf</td><td>TMM 2019</td><td></td></tr><tr><td>16</td><td>GPS: GROUP PEOPLE SEGMENTATION WITH DETAILED PART INFERENCE [oral]</td><td>Yue Liao, Si Liu*, Tianrui Hui, Chen Gao, Yao Sun, Hefei Ling, Bo Li</td><td>2019</td><td>paper_thumb&#x2F;QQ20190529102806.png</td><td>conference</td><td></td><td>paper&#x2F;1231-camera-ready.pdf</td><td>ICME 2019</td><td></td></tr><tr><td>18</td><td>Fine-grained Human-centric Tracklet Segmentation with Single Frame Supervision</td><td>Si Liu, Guanghui Ren, Yao Sun, Jinqiao Wang, Changhu Wang, Bo Li, Shuicheng Yan</td><td>2019</td><td>paper_thumb&#x2F;thumb_FVYh21c.png</td><td>journal</td><td></td><td>paper&#x2F;Fine-Grained_Human-Centric_Tracklet_Segmentation_with_Single_Frame_Supervision.pdf</td><td>TPAMI 2019</td><td></td></tr><tr><td>19</td><td>Enhanced Memory Network for Video Segmentation [1st Place in Youtube-VOS 2019]</td><td>Zhishan Zhou, Lejian Ren, Pengfei Xiong, Yifei Ji, Peisen Wang, Haoqiang Fan, Si Liu</td><td>2019</td><td>paper_thumb&#x2F;7366E78E-DD17-49A9-BD46-7EB69D0955DF_20191019173024.jpg</td><td>conference</td><td></td><td>paper&#x2F;iccvw_2019_videotrack.pdf</td><td>ICCV Workshop 2019</td><td></td></tr><tr><td>22</td><td>RGB-Infrared Cross-Modality Person Re-Identification via Joint Pixel and Feature Alignment</td><td>Guan’an Wang, Tianzhu Zhang, Jian Cheng, Si Liu, Yang Yang, Zengguang Hou</td><td>2019</td><td>paper_thumb&#x2F;12-1.jpg</td><td>conference</td><td></td><td>paper&#x2F;RGB-Infrared_Cross-Modality.pdf</td><td>ICCV 2019</td><td></td></tr><tr><td>23</td><td>PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer</td><td>Wentao Jiang, Si Liu, Chen Gao, Jie Cao, Ran He, Jiashi Feng, Shuicheng Yan</td><td>2020</td><td>paper_thumb&#x2F;psgan.jpg</td><td>conference</td><td></td><td>paper&#x2F;Jiang_PSGAN_Pose_and_Expression_Robust_Spatial-Aware_GAN_for_Customizable_Makeup_C_v1IxyXA.pdf</td><td>CVPR 2020 Oral</td><td></td></tr><tr><td>24</td><td>AdversarialNAS: Adversarial Neural Architecture Search for GANs</td><td>Chen Gao, Yunpeng Chen, Si Liu, Zhenxiong Tan, Shuicheng Yan</td><td>2020</td><td>paper_thumb&#x2F;AdversariaINAS_xyrdKh2.jpg</td><td>conference</td><td></td><td>paper&#x2F;AdversarialNAS_Adversarial_Neural_Architecture_Search_for_GANs_cAyDxa4.pdf</td><td>CVPR 2020 Oral</td><td></td></tr><tr><td>25</td><td>A Real-Time Cross-modality Correlation Filtering Method for Referring Expression Comprehension</td><td>Yue Liao, Si Liu , Guanbin Li ,Fei Wang ,Yanjie Chen ,Chen Qian ,Bo Li</td><td>2020</td><td>paper_thumb&#x2F;rcff.jpg</td><td>conference</td><td></td><td>paper&#x2F;Liao_A_Real-Time_Cross-Modality_Correlation_Filtering_Method_for_Referring_Expression.pdf</td><td>CVPR 2020</td><td></td></tr><tr><td>26</td><td>PPDM: Parallel Point Detection and Matching for Real-time Human-Object Interaction Detection</td><td>Yue Liao, Si Liu, Fei Wang, Yanjie Chen, Chen Qian, Jiashi Feng</td><td>2020</td><td>paper_thumb&#x2F;PPDM_thumb.png</td><td>conference</td><td></td><td>paper&#x2F;PPDM_Parallel_Point_Detection_and_Matching_for_Real-time_Human-Object.pdf</td><td>CVPR 2020</td><td></td></tr><tr><td>28</td><td>Scene Graph Generation with Hierarchical Context</td><td>Guanghui Ren, Lejian Ren, Yue Liao, Si Liu, Bo Li, Jizhong Han, Shuicheng Yan</td><td>2020</td><td>paper_thumb&#x2F;1.jpg</td><td>journal</td><td></td><td>paper&#x2F;FINAL_VERSION.PDF</td><td>TNNLS 2020</td><td></td></tr><tr><td>30</td><td>Rule-Guided Compositional Representation Learning on Knowledge GraphsRule-Guided Compositional Representation Learning on Knowledge Graphs</td><td>Guanglin Niu,Yongfei Zhang,Bo Li,Peng Cui,Si Liu,Jingyang Li,Xiaowei Zhang</td><td>2020</td><td>paper_thumb&#x2F;Rule-Guided.jpg</td><td>conference</td><td></td><td>paper&#x2F;5687-Article-Text-8912-1-10-20200513.pdf</td><td>AAAI 2020</td><td></td></tr><tr><td>31</td><td>Tree-Structured Policy based Progressive Reinforcement Learning for Temporally Language Grounding in Video</td><td>Jie Wu, Guanbin Li, Si Liu, Liang Lin</td><td>2020</td><td>paper_thumb&#x2F;Tree-.png</td><td>conference</td><td></td><td>paper&#x2F;2001.06680v1_npdqCiD.pdf</td><td>AAAI 2020</td><td></td></tr><tr><td>32</td><td>A weakly supervised method for makeup-invariant face verification</td><td>Yao Sun, Lejian Ren, Zhen Wei, Bin Liu, Yanlong Zhai, Si Liu</td><td>2017</td><td>paper_thumb&#x2F;A_weakly_supervised_method_for_makeup-invariant_face_verification.png</td><td>journal</td><td></td><td>paper&#x2F;A_weakly_supervised_method_for_makeup-invariant_face_verification.pdf</td><td>Pattern Recognition 2017</td><td></td></tr><tr><td>33</td><td>Adult Images and Videos Recognition by Deep Multi-Context Network and Fine-to-Coarse Strategy</td><td>Xinyu Ou, Hefei Ling, Han Yu, Si Liu</td><td>2017</td><td>paper_thumb&#x2F;Adult_Images_and_Videos_Recognition_by_Deep_Multi-Context_Network_and_Fine-t_gTpkniP.jpg</td><td>journal</td><td></td><td>paper&#x2F;Adult_Images_and_Videos_Recognition_by_Deep_Multi-Context_Network_and_Fine-to-Coar_caIRCIW.pdf</td><td>ACM Transactions on Intelligent Systems and Technology (TIST) 2017</td><td></td></tr><tr><td>34</td><td>Objectness Region Enhancement Networks for Scene Parsing</td><td>Xinyu Ou, Ping Li, Hefei Ling, Si Liu, Tianjiang Wang, Dan Li</td><td>2017</td><td>paper_thumb&#x2F;Objectness_Region_Enhancement_Networks_for_Scene_Parsing.jpg</td><td>journal</td><td></td><td>paper&#x2F;Objectness_Region_Enhancement_Networks_for_Scene_Parsing.pdf</td><td>Journal of Computer Science and Technology (JCST) 2017</td><td></td></tr><tr><td>35</td><td>Face Aging with Contextual Generative Adversarial Nets</td><td>Si Liu, Yao Sun, Wei Wang, Renda Bao, Defa Zhu, Xiangbo Zhu, and Shuicheng Yan</td><td>2017</td><td>paper_thumb&#x2F;Face_Aging_with_Contextual_Generative_Adversarial_Nets.png</td><td>conference</td><td></td><td>paper&#x2F;Face_Aging_with_Contextual_Generative_Adversarial_Nets.pdf</td><td>ACM MM 2017</td><td></td></tr><tr><td>36</td><td>Fast Deep Matting for Portrait Animation on Mobile Phone</td><td>Bingke Zhu, Yingying Chen, Si Liu, Bo Zhang, Jinqiao Wang and Ming Tang</td><td>2017</td><td>paper_thumb&#x2F;Fast_Deep_Matting_for_Portrait_Animation_on_Mobile_Phone.png</td><td>conference</td><td></td><td>paper&#x2F;Fast_Deep_Matting_for_Portrait_Animation_on_Mobile_Phone.pdf</td><td>ACM MM 2017</td><td></td></tr><tr><td>37</td><td>Magic-wall: Visualizing Room Decoration</td><td>Ting Liu, Yunchao Wei, Yao Zhao, Si Liu and Shikui Wei</td><td>2017</td><td>paper_thumb&#x2F;Magic-wall_Visualizing_Room_Decoration.png</td><td>conference</td><td></td><td>paper&#x2F;Magic-wall_Visualizing_Room_Decoration.pdf</td><td>ACM MM 2017</td><td></td></tr><tr><td>38</td><td>RSVP: A Real-Time Surveillance Video Parsing System with Single Frame Supervision</td><td>Han Yu, Guanghui Ren, Ruihe Qian, Yao Sun, Changhu Wang, Hanqing Lu and Si Liu</td><td>2017</td><td>paper_thumb&#x2F;rsvp-acmmm2017.png</td><td>conference</td><td></td><td>paper&#x2F;rsvp-acmmm2017.pdf</td><td>ACM MM 2017</td><td></td></tr><tr><td>39</td><td>Time Traveler: a real-time face aging system</td><td>Lejian Ren, Si Liu, Yao Sun, JianDong, Luoqi Liu and Shuicheng Yan</td><td>2017</td><td>paper_thumb&#x2F;Time_Traveler_a_real-time_face_aging_system_ACM_MM_2017.png</td><td>conference</td><td></td><td>paper&#x2F;Time_Traveler_a_real-time_face_aging_system_ACM_MM_2017.pdf</td><td>ACM MM 2017</td><td></td></tr><tr><td>40</td><td>Learning Adaptive Receptive Fields for Deep Image Parsing Network</td><td>Zhen Wei, Yao Sun, Jinqiao Wang, Hanjiang Lai, Si Liu</td><td>2017</td><td>paper_thumb&#x2F;Learning_Adaptive_Receptive_Fields_for_Deep_Image_Parsing_Network.png</td><td>conference</td><td></td><td>paper&#x2F;Learning_Adaptive_Receptive_Fields_for_Deep_Image_Parsing_Network.pdf</td><td>CVPR 2017</td><td></td></tr><tr><td>41</td><td>Surveillance Video Parsing with Single Frame Supervision</td><td>Si Liu, Changhu Wang, Ruihe Qian, Han Yu, Renda Bao, Yao Sun</td><td>2017</td><td>paper_thumb&#x2F;Surveillance_Video_Parsing_with_Single_Frame_Supervisionfinal_version.png</td><td>conference</td><td></td><td>paper&#x2F;Surveillance_Video_Parsing_with_Single_Frame_Supervisionfinal_version.pdf</td><td>CVPR 2017</td><td></td></tr><tr><td>42</td><td>Visual Attributes for Fashion Analytics</td><td>Si Liu, Lisa M. Brown, Qiang Chen, Junshi Huang, Luoqi Liu and Shuicheng Yan</td><td>2016</td><td>paper_thumb&#x2F;Visual_Attributes_for_Fashion_Analytics._Chapter_in_Visual_Attributes.png</td><td>book</td><td></td><td>paper&#x2F;Visual_Attributes_for_Fashion_Analytics._Chapter_in_Visual_Attributes.pdf</td><td>Chapter in Visual Attributes. R. Feris, C. Lampert, and D. Parikh, Editors. Springer. To appear, 2016</td><td></td></tr><tr><td>44</td><td>Beauty eMakeup: a Deep Makeup Transfer System</td><td>Xinyu Ou, Si Liu, Xiaochun Cao, Hefei Ling</td><td>2016</td><td>paper_thumb&#x2F;Beauty_eMakeup_a_Deep_Makeup_Transfer_System.png</td><td>conference</td><td></td><td>paper&#x2F;Beauty_eMakeup_a_Deep_Makeup_Transfer_System.pdf</td><td>ACM MM demo</td><td></td></tr><tr><td>45</td><td>Single Image Dehazing via Multi-Scale Convolutional Neural Networks</td><td>Wenqi Ren, Si Liu, Hua Zhang, Jianshan Pan, Xiaochun Cao, Ming-Hsuan Yang</td><td>2016</td><td>paper_thumb&#x2F;Single_Image_Dehazing_via_Multi-Scale_Convolutional_Neural_Networks.png</td><td>conference</td><td></td><td>paper&#x2F;Single_Image_Dehazing_via_Multi-Scale_Convolutional_Neural_Networks.pdf</td><td>ECCV 2016</td><td></td></tr><tr><td>46</td><td>Deep Multi-Context Network for Fine-Grained Visual Recognition</td><td>Xinyu Ou, Zhen Wei, Si Liu, Xiaochun Cao, Hefei Ling</td><td>2016</td><td>paper_thumb&#x2F;Deep_Multi-Context_Network_for_Fine-Grained_Visual_Recognition.png</td><td>conference</td><td></td><td>paper&#x2F;Deep_Multi-Context_Network_for_Fine-Grained_Visual_Recognition.pdf</td><td>ICME 2016, Grand Challenge</td><td></td></tr><tr><td>47</td><td>Makeup like a superstar: Deep Localized Makeup Transfer Network</td><td>Si Liu, Xinyu Ou, Ruihe Qian, Wei Wang and Xiaochun Cao</td><td>2016</td><td>paper_thumb&#x2F;Makeup_like_a_superstar_Deep_Localized_Makeup_Transfer_Network.png</td><td>conference</td><td></td><td>paper&#x2F;Makeup_like_a_superstar_Deep_Localized_Makeup_Transfer_Network.pdf</td><td>IJCAI 2016</td><td></td></tr><tr><td>48</td><td>SketchNet: Sketch Classification with Web Images</td><td>Hua Zhang, Si Liu, Changqing Zhang, Wenqi Ren, Xiaochun Cao</td><td>2016</td><td>paper_thumb&#x2F;SketchNet_Sketch_Classification_with_Web_Images.png</td><td>conference</td><td></td><td>paper&#x2F;SketchNet_Sketch_Classification_with_Web_Images.pdf</td><td>CVPR 2016</td><td></td></tr><tr><td>49</td><td>Structural Correlation Filter for Robust Visual Tracking</td><td>Si Liu, Tianzhu Zhang, Changsheng Xu, and Xiaochun Cao</td><td>2016</td><td>paper_thumb&#x2F;Structural_Correlation_Filter_for_Robust_Visual_Tracking.png</td><td>conference</td><td></td><td>paper&#x2F;Structural_Correlation_Filter_for_Robust_Visual_Tracking.pdf</td><td>CVPR 2016</td><td></td></tr><tr><td>50</td><td>Robust Visual Tracking via Exclusive Context Modeling</td><td>Tianzhu Zhang,Bernard Ghanem,Si Liu,Changsheng Xu,Narendra Ahuja</td><td>2015</td><td>paper_thumb&#x2F;Robust_Visual_Tracking_via_Exclusive.jpg</td><td>journal</td><td></td><td>paper&#x2F;Robust_Visual_Tracking_via_Exclusive_Context_Modeling.pdf</td><td>IEEE Transactions on Cybernetics</td><td></td></tr><tr><td>51</td><td>Deep Human Parsing with Active Template Regression</td><td>Xiaodan Liang, Si Liu, Xiaohui Shen, Jianchao Yang, Luoqi Liu, Liang Lin, Shuicheng Yan</td><td>2015</td><td>paper_thumb&#x2F;Deep_Human_Parsing_with_Active_Template_Regression.png</td><td>journal</td><td></td><td>paper&#x2F;Deep_Human_Parsing_with_Active_Template_Regression.pdf</td><td>IEEE Transactions on Pattern Analysis and Machine Intelligence, (TPAMI) 2015</td><td></td></tr><tr><td>52</td><td>Fashion Parsing With Video Context</td><td>Si Liu, Xiaodan Liang, Luoqi Liu, Liang Lin, Ke Lv, Xiaochun Cao , Shuicheng Yan</td><td>2015</td><td>paper_thumb&#x2F;Fashion_Parsing_With_Video_Context.png</td><td>journal</td><td></td><td>paper&#x2F;Fashion_Parsing_With_Video_Context.pdf</td><td>IEEE Transactions on Multimedia (TMM) 2015</td><td></td></tr><tr><td>53</td><td>Matching-CNN Meets KNN: Quasi-Parametric Human Parsing</td><td>Si Liu, Xiaodan Liang, Luoqi Liu, Xiaohui Shen, Jianchao Yang, Changsheng Xu, Xiaochun Cao, Shuicheng Yan</td><td>2015</td><td>paper_thumb&#x2F;Matching-CNN_Meets_KNN_Quasi-Parametric_Human_Parsing.png</td><td>conference</td><td></td><td>paper&#x2F;Matching-CNN_Meets_KNN_Quasi-Parametric_Human_Parsing.pdf</td><td>CVPR 2015</td><td></td></tr><tr><td>54</td><td>Towards Computational Baby Learning: A Weakly-Supervised Approach for Object Detection</td><td>Xiaodan Liang, Si Liu, Yunchao Wei, Luoqi Liu, Liang Lin, Shuicheng Yan</td><td>2015</td><td>paper_thumb&#x2F;Towards_Computational_Baby_Learning_A_Weakly-Supervised_Approach_for_Object__WQpwKRe.png</td><td>conference</td><td></td><td>paper&#x2F;Towards_Computational_Baby_Learning_A_Weakly-Supervised_Approach_for_Object_Detection.pdf</td><td>ICCV 2015</td><td></td></tr><tr><td>55</td><td>SLED: Semantic Label Embedding Dictionary Representation for Multi-label Image Annotation</td><td>Xiaochun Cao, Hua Zhang, Xiaojie Guo, Si Liu and Dan Meng</td><td>2015</td><td>paper_thumb&#x2F;SLED_Semantic_Label_Embedding_Dictionary.jpg</td><td>journal</td><td></td><td>paper&#x2F;SLED_Semantic_Label_Embedding_Dictionary_Representation_for_Multilabel_Image_Annotation_.pdf</td><td>IEEE Trans. on Image Processing (TIP), 2015</td><td></td></tr><tr><td>56</td><td>Deep People Counting in Extremely Dense Crowds</td><td>Chuan Wang, Hua Zhang, Yang Liang, Si Liu, Xiaochun Cao</td><td>2015</td><td>paper_thumb&#x2F;Deep_People_Counting_in_Extremely_Dense_Crowds.jpg</td><td>conference</td><td></td><td>paper&#x2F;Deep_People_Counting_in_Extremely_Dense_Crowds.pdf</td><td>ACM MM 2015</td><td></td></tr><tr><td>57</td><td>Diversity-induced Multiview Subspace Clustering</td><td>Xiaochun Cao, Changqing Zhang, Huazhu Fu, Si Liu</td><td>2015</td><td>paper_thumb&#x2F;Diversity-induced_Multi-view_Subspace_Clustering.jpg</td><td>conference</td><td></td><td>paper&#x2F;Diversity-Induced_Multi-View_Subspace_2015_CVPR_paper.pdf</td><td>CVPR 2015</td><td></td></tr><tr><td>58</td><td>Structural Sparse Tracking</td><td>Tianzhu Zhang, Si Liu, Changsheng Xu, Shuicheng Yan, Narendra Ahuja, Bernard Ghanem, Ming-Hsuan Yang</td><td>2015</td><td>paper_thumb&#x2F;Structural_Sparse_Tracking.jpg</td><td>conference</td><td></td><td>paper&#x2F;Structural_Sparse_Tracking_2015_CVPR_paper.pdf</td><td>CVPR 2015</td><td></td></tr><tr><td>59</td><td>Low-Rank Tensor Constrained Multiview Subspace Clustering</td><td>Changqing Zhang, Huazhu Fu, Si Liu, Guangcan Liu, Xiaochun, Cao</td><td>2015</td><td>paper_thumb&#x2F;Low-Rank_Tensor_Constrained_Multiview_Subspace_Clustering.jpg</td><td>conference</td><td></td><td>paper&#x2F;Low-Rank_Tensor_Constrained_ICCV_2015_paper.pdf</td><td>ICCV 2015</td><td></td></tr><tr><td>60</td><td>Human Parsing With Contextualized Convolutional Neural Network</td><td>Xiaodan Liang, Chunyan Xu, Xiaohui Shen, Jianchao Yang, Si Liu, Jinhui Tang, Liang Lin, Shuicheng Yan</td><td>2015</td><td>paper_thumb&#x2F;Human_Parsing_With.jpg</td><td>conference</td><td></td><td>paper&#x2F;Human_Parsing_With_ICCV_2015_paper.pdf</td><td>ICCV 2015</td><td></td></tr><tr><td>61</td><td>Robust Visual Tracking via Consistent Low-Rank Sparse Learning</td><td>Tianzhu Zhang, Si Liu, Narendra Ahuja, Ming-Hsuan Yang, Bernard Ghanem</td><td>2015</td><td>paper_thumb&#x2F;Robust_Visual_Tracking_Via_Consistent_Low-Rank_Sparse.jpg</td><td>journal</td><td></td><td>paper&#x2F;Robust_Visual_Tracking_Via_Consistent_Low-Rank_Sparse.pdf</td><td>International Journal of Computer Vision (IJCV), 2014</td><td></td></tr><tr><td>62</td><td>Fashion Parsing with Weak Color-Category Labels</td><td>Si Liu, Jiashi Feng, Csaba Domokos, Junshi Huang, Zhenzhen Hu, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;Fashion_Parsing_With_Weak_Color-Category_Labels.png</td><td>journal</td><td></td><td>paper&#x2F;Fashion_Parsing_With_Weak_Color-Category_Labels.pdf</td><td>IEEE Transactions on Multimedia (TMM), 2014</td><td></td></tr><tr><td>63</td><td>Fashion Analysis: Current Techniques and Future Directions</td><td>Si Liu, Luoqi Liu, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;Fashion_Analysis.jpg</td><td>journal</td><td></td><td>paper&#x2F;Fashion_Analysis_Current_Techniques_and_Future_Directions.pdf</td><td>IEEE MultiMedia 21(2): 72-79 (2014)</td><td></td></tr><tr><td>64</td><td>Snap &amp; Play: Auto-Generate Personalized Find-the-Difference Game</td><td>Si Liu, Qiang Chen, Shuicheng Yan, Changsheng Xu, Hanqing Lu</td><td>2014</td><td>paper_thumb&#x2F;J_Snap__Play_Auto-Generated_Personalized_Find-the-Difference_Game.jpg</td><td>journal</td><td></td><td>paper&#x2F;J_Snap__Play_Auto-Generated_Personalized_Find-the-Difference_Game.pdf</td><td>IEEE Transactions on Image Processing (TIST), 2014</td><td></td></tr><tr><td>65</td><td>Wow! You Are So Beautiful Today!</td><td>Luoqi Liu, Jun¬liang Xing, Si Liu, Hui Xu, Xi Zhou, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;J_Wow_You_Are_So_Beautiful_Today.jpg</td><td>journal</td><td></td><td>paper&#x2F;J_Wow_You_Are_So_Beautiful_Today.pdf</td><td>Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP), 2014</td><td></td></tr><tr><td>66</td><td>Weakly-Supervised Graph Propagation Towards Collective Image Parsing</td><td>Si Liu, Shuicheng Yan, Tianzhu Zhang, Changsheng Xu, Jing Liu, Hanqing Lu</td><td>2014</td><td>paper_thumb&#x2F;Weakly_Supervised_Graph_Propagation_Towards_Collective_Image_Parsing.png</td><td>journal</td><td></td><td>paper&#x2F;Weakly_Supervised_Graph_Propagation_Towards_Collective_Image_Parsing.pdf</td><td>IEEE Transactions on Multimedia (TMM), Volume: 14, Issue: 2, Page(s): 361-373, 2012</td><td></td></tr><tr><td>67</td><td>PicWords: Render a Picture by Packing Keywords</td><td>Zhenzhen Hu, Si Liu, Jianguo Jiang, Richang Hong, Meng Wang, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;PicWords_Render_a_Picture_by_Packing_Keywords.png</td><td>journal</td><td></td><td>paper&#x2F;PicWords_Render_a_Picture_by_Packing_Keywords.pdf</td><td>IEEE Transactions on Multimedia (TMM) 16(4): 1156-1164 (2014)</td><td></td></tr><tr><td>68</td><td>Circle &amp; Search: Attribute-aware Shoe Retrieval</td><td>Junshi Huang, Si Liu, Junliang Xing, Tao Mei, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;Circle__Search_Attribute-Aware_Shoe_Retrieval.png</td><td>journal</td><td></td><td>paper&#x2F;Circle__Search_Attribute-Aware_Shoe_Retrieval.pdf</td><td>Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP), 2014</td><td></td></tr><tr><td>69</td><td>Clothing Attributes Assisted Person Re-identification</td><td>Annan Li, Luoqi Liu, Kang Wang, Si Liu, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;Clothing_Attributes_Assisted_Person_Reidentification.jpg</td><td>journal</td><td></td><td>paper&#x2F;Clothing_Attributes_Assisted_Person_Reidentification.pdf</td><td>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT). In 2014</td><td></td></tr><tr><td>70</td><td>Towards Decrypting Attractiveness via Multi-Modality Cues</td><td>Tam V. Nguyen, Si Liu, Bingbing Ni, Jun Tan, Yong Rui, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;Towards_Decrypting_Attractiveness_via_Multi-Modality_Cues.png</td><td>journal</td><td></td><td>paper&#x2F;Towards_Decrypting_Attractiveness_via_Multi-Modality_Cues.pdf</td><td>Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP), 2013</td><td></td></tr><tr><td>71</td><td>Robust Visual Tracking via Structured Multi-Task Sparse Learning</td><td>Tianzhu Zhang, Bernard Ghanem, Si Liu*, Narendra Ahuja</td><td>2014</td><td>paper_thumb&#x2F;Robust_Visual_Tracking_via_Structured_Multi-Task_Sparse.jpg</td><td>journal</td><td></td><td>paper&#x2F;Robust_Visual_Tracking_via_Structured_Multi-Task_Sparse_Learning.pdf</td><td>International Journal of Computer Vision (IJCV), Volume: 101, Issue: 2, Page(s): 367-383, 2012</td><td></td></tr><tr><td>72</td><td>A Generic Framework for Video Annotation via Semi-supervised Learning</td><td>Tianzhu Zhang, Changsheng Xu, Guangyu Zhu, Si Liu and Hanqing Lu</td><td>2014</td><td>paper_thumb&#x2F;A_Generic_Framework_for_Video_Annotation.jpg</td><td>journal</td><td></td><td>paper&#x2F;A_Generic_Framework_for_Video_Annotation_via_Semi-Supervised_Learning.pdf</td><td>IEEE Transactions on Multimedia (TMM), Volume: 14, Issue: 4, Page(s): 1206-1219, 2012</td><td></td></tr><tr><td>73</td><td>Boosted Exemplar Learning for Action Recognition and Annotation</td><td>Tianzhu Zhang, Jing Liu, Si Liu, Changsheng Xu, and Hanqing Lu</td><td>2014</td><td>paper_thumb&#x2F;Boosted_Exemplar_Learning_for_Action.jpg</td><td>journal</td><td></td><td>paper&#x2F;Boosted_Exemplar_Learning_for_Action_Recognition_and_Annotation.pdf</td><td>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), Volume: 21, Issue: 7, Page(s): 853-866, 2011</td><td></td></tr><tr><td>74</td><td>Fashion Parsing with Video Context</td><td>Si Liu, Xiaodan Liang, Luoqi Liu, Liang Lin, Ke Lv, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;Fashion_Parsing_with_Video_Context.png</td><td>conference</td><td></td><td>paper&#x2F;Fashion_Parsing_with_Video_Context.pdf</td><td>ACM MM, 2014</td><td></td></tr><tr><td>75</td><td>Hi, magic closet, tell me what to wear!</td><td>Si Liu, Tam V. Nguyen, Jiashi Feng, Meng Wang, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;Hi_Magic_Closet_Tell_Me_What_to_Wear.png</td><td>conference</td><td></td><td>paper&#x2F;Hi_Magic_Closet_Tell_Me_What_to_Wear.pdf</td><td>ACM MM, 2012, (Best Technical Demonstration Award)</td><td></td></tr><tr><td>76</td><td>Hi, magic closet, tell me what to wear!</td><td>Si Liu, Jiashi Feng, Zheng Song, Tianzhu Zhang, Hanqing Lu, Changsheng Xu, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;Hi_Magic_Closet_Tell_Me_What_to_Wear_POK8tNT.png</td><td>conference</td><td></td><td>paper&#x2F;Hi_Magic_Closet_Tell_Me_What_to_Wear_XXHoJYp.pdf</td><td>ACM MM, 2012</td><td></td></tr><tr><td>77</td><td>Street-to-Shop: Cross-Scenario Clothing Retrieval via Human Part Alignment and Auxiliary Set</td><td>Si Liu, Zheng Song, Guangcan Liu, Shuicheng Yan, Changsheng Xu, Hanqing Lu</td><td>2014</td><td>paper_thumb&#x2F;Street-to-shop_Cross-scenario_clothing_retrieval_via_parts_alignment_and_aux_h9c13aZ.png</td><td>conference</td><td></td><td>paper&#x2F;Street-to-shop_Cross-scenario_clothing_retrieval_via_parts_alignment_and_auxiliary_set.pdf</td><td>CVPR, 2012, (Oral, 2.5%)</td><td></td></tr><tr><td>78</td><td>Street-to-shop: cross-scenario clothing retrieval via parts alignment and auxiliary set</td><td>Si Liu, Zheng Song, Meng Wang, Changsheng Xu, Hanqing Lu, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;Street-to-shop_Cross-scenario_clothing_retrieval_via_parts_alignment_and_aux_0IQe1lf.png</td><td>conference</td><td></td><td>paper&#x2F;Street-to-shop_Cross-scenario_clothing_retrieval_via_parts_alignment_and_auxiliary_GvthsQu.pdf</td><td>ACM MM demo, 2012</td><td></td></tr><tr><td>79</td><td>Size Adaptive Selection of Most Informative Features</td><td>Si Liu, Hairong Liu, Shuicheng Yan, Longin Latecki, Changsheng Xu, Hanqing Lu</td><td>2014</td><td>paper_thumb&#x2F;Size_Adaptive_Selection_of_Most_Informative_Features.jpg</td><td>conference</td><td></td><td>paper&#x2F;Size_Adaptive_Selection_of_Most_Informative_Features.pdf</td><td>AAAI 2011, (Oral, 4.8% acceptance rate)</td><td></td></tr><tr><td>80</td><td>Snap &amp; Play: Auto-generate Personalized Find-the-Difference Mobile Game</td><td>Si Liu, Qiang Chen, Shuicheng Yan, Changsheng Xu, Hanqing Lu</td><td>2014</td><td>paper_thumb&#x2F;Snap__Play_Auto-Generated_Personalized_Find-the-Difference_Game_aIICHAf.jpg</td><td>conference</td><td></td><td>paper&#x2F;Snap__Play_Auto-Generate_Personalized_Find-the-Difference_Mobile_Game.pdf</td><td>ACM MM, 2011</td><td></td></tr><tr><td>81</td><td>Puzzle Search: Image Retrieval and Ranking with Consistent Reconstruction of Multi-Attribute Queries</td><td>Xiaochun Cao, Hua Zhang, Xiaojie Guo, Si Liu, and Xiaowu Chen</td><td>2014</td><td>paper_thumb&#x2F;Image_Retrieval_and_Ranking_via_Consistently.jpg</td><td>conference</td><td></td><td>paper&#x2F;Image_Retrieval_and_Ranking_via_Consistently.pdf</td><td>ECCV, 2014</td><td></td></tr><tr><td>82</td><td>Wow! you are so beautiful today!</td><td>Luoqi Liu, Hui Xu, Junliang Xing, Si Liu, Xi Zhou, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;C_Wow_You_Are_So_Beautiful_Today.jpg</td><td>conference</td><td></td><td>paper&#x2F;C_Wow_You_Are_So_Beautiful_Today.pdf</td><td>ACM MM, 2013. (Best Paper Award)</td><td></td></tr><tr><td>83</td><td>eHeritage of shadow puppetry: creation and manipulation</td><td>Min Lin, Zhenzhen Hu, Si Liu, Meng Wang, Richang Hong, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;eHeritage_of_Shadow_Puppetry_Creation_and_Manipulation.jpg</td><td>conference</td><td></td><td>paper&#x2F;eHeritage_of_Shadow_Puppetry_Creation_and_Manipulation.pdf</td><td>ACM MM, 2013. (Oral, 20% acceptance rate) and a ACM DEMO</td><td></td></tr><tr><td>84</td><td>Wow! you are so beautiful today!</td><td>Luoqi Liu, Hui Xu, Si Liu, Junliang Xing, Xi Zhou, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;Wow_you_are_so_beautiful_today_aQwuKk6.png</td><td>conference</td><td></td><td>paper&#x2F;Wow_you_are_so_beautiful_today_qbbrP2W.pdf</td><td>ACM MM demo, 2013</td><td></td></tr><tr><td>85</td><td>Low-Rank Sparse Coding for Image Classification</td><td>Tianzhu Zhang, Bernard Ghanem, Si Liu, Changsheng Xu , Narendra Ahuja</td><td>2014</td><td>paper_thumb&#x2F;Low-Rank_Sparse_Coding_for_Image_Classification.jpg</td><td>conference</td><td></td><td>paper&#x2F;Low-Rank_Sparse_Coding_for_Image_Classification.pdf</td><td>ICCV, 2013</td><td></td></tr><tr><td>86</td><td>SYM-FISH: A Symmetry-aware Flip Invariant Sketch Histogram Shape Descriptor</td><td>Xiaochun Cao, Hua Zhang, Si Liu, Xiaojie Guo</td><td>2014</td><td>paper_thumb&#x2F;SYM-FISH_A_Symmetry-aware_Flip_Invariant.jpg</td><td>conference</td><td></td><td>paper&#x2F;SYM-FISH_A_Symmetry-Aware_2013_ICCV_paper.pdf</td><td>ICCV 2013</td><td></td></tr><tr><td>87</td><td>Sense beauty via face, dressing, and&#x2F;or voice</td><td>Tam V. Nguyen, Si Liu, Bingbing Ni, Jun Tan, Yong Rui, Shuicheng Yan</td><td>2014</td><td>paper_thumb&#x2F;Sense_Beauty_via_Face_Dressing_and_or_Voice.png</td><td>conference</td><td></td><td>paper&#x2F;Sense_Beauty_via_Face_Dressing_and_or_Voice.pdf</td><td>ACM MM 2012, (full paper)</td><td></td></tr><tr><td>88</td><td>Low-Rank Sparse Learning for Robust Visual Tracking</td><td>Tianzhu Zhang, Bernard Ghanem, Si Liu, Narendra Ahuja</td><td>2014</td><td>paper_thumb&#x2F;Low-Rank_Sparse_Learning.jpg</td><td>conference</td><td></td><td>paper&#x2F;Low-Rank_Sparse_Learning.pdf</td><td>ECCV, 2012</td><td></td></tr><tr><td>89</td><td>Robust Visual Tracking via Multi-Task Sparse Learning</td><td>Tianzhu Zhang, Bernard Ghanem, Si Liu, Narendra Ahuja</td><td>2014</td><td>paper_thumb&#x2F;Robust_Visual_Tracking_via_Multi-Task_Sparse_Learning.jpg</td><td>conference</td><td></td><td>paper&#x2F;Robust_Visual_Tracking_via_Multi-Task_Sparse_Learning.pdf</td><td>CVPR, 2012, (Oral, 2.5%)</td><td></td></tr><tr><td>90</td><td>A Generic Framework for Event Detection in Various Video Domain</td><td>Tianzhu Zhang, Changsheng Xu, Guangyu Zhu, Si Liu, Hanqing Lu</td><td>2014</td><td>paper_thumb&#x2F;A_Generic_Framework_for_Event_Detection.jpg</td><td>conference</td><td></td><td>paper&#x2F;A_Generic_Framework_for_Event_Detection_in_Various_Video_Domains_.pdf</td><td>ACM MM, 2010</td><td></td></tr><tr><td>91</td><td>Finding Images by Dialoguing with Image</td><td>Lejian Ren,Si Liu,Han Huang,Jizhong Han,Shuicheng Yan,Bo Li</td><td>2019</td><td>paper_thumb&#x2F;mm_2019.jpg</td><td>conference</td><td></td><td>paper&#x2F;MM-sigconf_2hgqRdx_a9Laf8h.pdf</td><td>ACM MM 2019</td><td></td></tr><tr><td>92</td><td>Referring Image Segmentation via Cross-Modal Progressive Comprehension</td><td>Shaofei Huang*, Tianrui Hui*, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, Bo Li. (*Equal contribution)</td><td>2020</td><td>paper_thumb&#x2F;referring.jpg</td><td>conference</td><td></td><td>paper&#x2F;Huang_Referring_Image_Segmentation_via_Cross-Modal_Progressive_Comprehension_CVPR__ZQg2PZN.pdf</td><td>CVPR 2020</td><td></td></tr><tr><td>93</td><td>Linguistic Structure Guided Context Modeling for Referring Image Segmentation</td><td>Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, Jizhong Han</td><td>2020</td><td>paper_thumb&#x2F;Linguistic_Structure_Guided_Context_Modeling_for_Referring_Image_Segmentation.png</td><td>conference</td><td></td><td>paper&#x2F;Linguistic_Structure_Guided_Context_Modeling_for_Referring_Image_Segmentation_2DmaQ5o.pdf</td><td>ECCV 2020</td><td></td></tr><tr><td>94</td><td>ORDNet: Capturing Omni-Range Dependencies for Scene Parsing</td><td>Shaofei Huang, si liu*, Tianrui Hui, Jizhong Han, Bo Li, Jiashi Feng, Shuicheng Yan</td><td>2020</td><td>paper_thumb&#x2F;ord_net.jpg</td><td>journal</td><td></td><td>paper&#x2F;ORDNet_Capturing_Omni-Range_Dependencies_for_Scene_Parsing.pdf</td><td>TIP</td><td></td></tr><tr><td>95</td><td>InteractGAN: Learning to Generate Human-Object Interaction</td><td>Chen Gao, si liu*, Defa Zhu, Quan Liu, Jie Cao, Haoqian He, Ran He, Shuicheng Yan</td><td>2020</td><td>paper_thumb&#x2F;2020_mm_gc_LnaNcja.png</td><td>Conference</td><td></td><td>paper&#x2F;3394171.3413854.pdf</td><td>acm mm 2020</td><td></td></tr><tr><td>96</td><td>Cross-Modal Omni Interaction Modeling for Phrase Grounding</td><td>Tianyu Yu, Tianrui Hui, Zhihao Yu, Yue Liao, Sansi Yu, Faxi Zhang, si liu*</td><td>2020</td><td>paper_thumb&#x2F;2020_mm_yty_4NU2vlb.png</td><td>Conference</td><td></td><td>paper&#x2F;3394171.3413846.pdf</td><td>acm mm 2020</td><td></td></tr><tr><td>97</td><td>Confidence-aware Non-repetitive Multimodal Transformers for TextCaps</td><td>Zhaokai Wang, Renda Bao, Qi Wu, Si Liu*</td><td>2021</td><td>paper_thumb&#x2F;wzk_aaai2021.png</td><td>conference</td><td></td><td>paper&#x2F;Confidence-aware_Non-repetitive_Multimodal_Transformers_for_TextCaps.pdf</td><td>AAAI 2021</td><td></td></tr><tr><td>98</td><td>Attentive Excitation and Aggregation for Bilingual Referring Image Segmentation</td><td>Qianli Zhou#, Tianrui Hui#, Rong Wang*, Haimiao Hu, Si Liu*(#Equal contribution)</td><td>2021</td><td>paper_thumb&#x2F;TIST.png</td><td>journal</td><td></td><td>paper&#x2F;Attentive_Excitation_and_Aggregation_for_Bilingual_Referring_Image_Segmentation.pdf</td><td>ACM Transactions on Intelligent Systems and Technology (TIST) 2021</td><td></td></tr><tr><td>99</td><td>Discriminative Triad Matching and Reconstruction for Weakly Referring Expression Grounding</td><td>Mingjie Sun, Jimin Xiao, Enggee Lim, Si Liu, John Yannis Goulermas</td><td>2021</td><td>paper_thumb&#x2F;Discriminative_Triad_Matching_and_Reconstruction_for_Weakly_Referring_Expres_9LVUYAK.png</td><td>journal</td><td></td><td>paper&#x2F;Discriminative_Triad_Matching_and_Reconstruction_for_Weakly_Referring_Expression_G_pTaiVPR.pdf</td><td>TPAMI 2021</td><td></td></tr><tr><td>100</td><td>Reformulating HOl Detection as Adaptive Set Prediction</td><td>Mingfei Chen#, Yue Liao#, Si Liu*, Zhiyuan Chen, Fei Wang, Chen Qian</td><td>2021</td><td>paper_thumb&#x2F;Reformulating_HOI_Detection_as_Adaptive_Set_Prediction.png</td><td>conference</td><td></td><td>paper&#x2F;Reformulating_HOI_Detection_as_Adaptive_Set_Prediction.pdf</td><td>CVPR 2021</td><td></td></tr><tr><td>101</td><td>Room-and-Object Aware Knowledge Reasoning for Remote Embodied Referring Expression</td><td>Chen Gao#, Jinyu Chen#, Si Liu*, Luting Wang, Qiong Zhang, Qi Wu</td><td>2021</td><td>paper_thumb&#x2F;Room-and-object_aware_knowledge_reasoning_for_remote_embodied_referring_expression.png</td><td>conference</td><td></td><td>paper&#x2F;Room-and-object_aware_knowledge_reasoning_for_remote_embodied_referring_expression_fTSOgr0.pdf</td><td>CVPR 2021 Oral</td><td></td></tr><tr><td>102</td><td>Collaborative Spatial-Temporal Modeling for Language-Queried Video Actor Segmentation</td><td>Tianrui Hui#, Shaofei Huang#, Si Liu*, Zihan Ding, Guanbin Li, Wenguan Wang, Jizhong Han, Fei Wang</td><td>2021</td><td>paper_thumb&#x2F;Collaborative_Spatial-Temporal_Modeling_for_Language-Queried_Video_Actor_Seg_K3ppyaw.png</td><td>conference</td><td></td><td>paper&#x2F;Hui_Collaborative_Spatial-Temporal_Modeling_for_Language-Queried_Video_Actor_Segmentation.pdf</td><td>CVPR 2021</td><td></td></tr><tr><td>103</td><td>General lnstance Distillation for Object Detection</td><td>Xing Dai#, Zeren Jiang#, Zhao Wu, Yiping Bao, Zhicheng Wang, Si Liu, Erjin Zhou</td><td>2021</td><td>paper_thumb&#x2F;General_Instance_Distillation_for_Object_Detection.png</td><td>conference</td><td></td><td>paper&#x2F;General_Instance_Distillation_for_Object_Detection.pdf</td><td>CVPR 2021</td><td></td></tr><tr><td>104</td><td>Differentiable Multi-Granularity Human Representation Learning for Instance-Aware Human Semantic Parsing</td><td>Tianfei Zhou, Wengguan Wang*, Si Liu, Yi Yang, Luc Van Gool</td><td>2021</td><td>paper_thumb&#x2F;Differentiable_Multi-Granularity_Human_Representation_Learni.png</td><td>conference</td><td></td><td>paper&#x2F;Differentiable_Multi-Granularity_Human_Representation_Learning_for.pdf</td><td>CVPR 2021 Oral</td><td></td></tr><tr><td>105</td><td>Human-centric Relation Segmentation: Dataset and Solution</td><td>Si Liu, Zitian Wang, Yulu Gao, Lejian Ren, Yue Liao, Guanghui Ren, Bo Li, Shuicheng Yan</td><td>2021</td><td>paper_thumb&#x2F;pic.png</td><td>journal</td><td></td><td>paper&#x2F;Human-centric_Relation_Segmentation_Dataset_and_Solution_jBmLebt.pdf</td><td>TPAMI 2021</td><td></td></tr><tr><td>106</td><td>Cross-Modal Progressive Comprehension for Referring Segmentation</td><td>Si Liu, Tianrui Hui, Shaofei Huang, Yunchao Wei, Bo Li, Guanbin Li*</td><td>2021</td><td>paper_thumb&#x2F;cmpc.png</td><td>journal</td><td></td><td>paper&#x2F;Cross-Modal_Progressive_Comprehension_for_Referring_Segmentation_tpD1psE.pdf</td><td>TPAMI 2021</td><td></td></tr><tr><td>107</td><td>PSGAN++: Robust Detail-Preserving Makeup Transfer and Removal</td><td>Si Liu, Wentao Jiang, Chen Gao, Ran He*, Jiashi Feng, Bo Li, Shuicheng Yan</td><td>2021</td><td>paper_thumb&#x2F;psgan.png</td><td>journal</td><td></td><td>paper&#x2F;PSGAN_Robust_Detail-Preserving_Makeup_Transfer_and_Removal_KFnd9Hv.pdf</td><td>TPAMI 2021</td><td></td></tr><tr><td>108</td><td>Human-centric Spatio-Temporal Video Grounding With Visual Transformers</td><td>Zongheng Tang, Yue Liao, Si Liu*, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, Dong Xu</td><td>2021</td><td>paper_thumb&#x2F;tcsvt.png</td><td>journal</td><td></td><td>paper&#x2F;tcsvt.pdf</td><td>TCSVT 2021</td><td></td></tr><tr><td>109</td><td>TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding</td><td>Dailan He#, Yusheng Zhao#, Junyu Luo#, Tianrui Hui, Shaofei Huang, Anxi Zhang, Si Liu*</td><td>2021</td><td>paper_thumb&#x2F;transrefer3D.png</td><td>conference</td><td></td><td>paper&#x2F;3474085.3475397.pdf</td><td>ACM MM 2021</td><td></td></tr><tr><td>110</td><td>Video Background Music Generation with Controllable Music Transformer</td><td>Shangzhe Di#, Zeren Jiang#, Si Liu*, Zhaokai Wang, Leyan Zhu, Zexin He, Hongming Liu, Shuicheng Yan</td><td>2021</td><td>paper_thumb&#x2F;mm.png</td><td>conference</td><td></td><td>paper&#x2F;3474085.3475195.pdf</td><td>ACM MM 2021 Best Paper</td><td></td></tr><tr><td>111</td><td>Language-Guided Global Image Editing via Cross-Modal Cyclic Mechanism</td><td>Wentao Jiang, Ning Xu, Jiayun Wang, Chen Gao,  Jing Shi, Zhe Lin, Si Liu*</td><td>2021</td><td>paper_thumb&#x2F;iccv.png</td><td>conference</td><td></td><td>paper&#x2F;Jiang_Language-Guided_Global_Image_Editing_via_Cross-Modal_Cyclic_Mechanism_ICCV_2_mC0jJUU.pdf</td><td>ICCV 2021</td><td></td></tr><tr><td>112</td><td>Mining the Benefits of Two-stage and One-stage HOI Detection</td><td>Aixi Zhang#, Yue Liao#, Si Liu*, Miao Lu, Yongliang Wang, Chen Gao, Xiaobo Li</td><td>2021</td><td>paper_thumb&#x2F;nips.png</td><td>conference</td><td></td><td>paper&#x2F;2108.05077.pdf</td><td>NeurIPS 2021</td><td></td></tr><tr><td>113</td><td>Language-Bridged Spatial-Temporal Interaction for Referring Video Object Segmentation</td><td>Zihan Ding, Tianrui Hui, Junshi Huang, Xiaoming Wei, Jizhong Han, Si Liu*</td><td>2022</td><td>paper_thumb&#x2F;lbdt.png</td><td>conference</td><td>https:&#x2F;&#x2F;github.com&#x2F;dzh19990407&#x2F;LBDT</td><td>paper&#x2F;Ding_Language-Bridged_Spatial-Temporal_Interaction_for_Referring_Video_Object_Segmentation.pdf</td><td>CVPR 2022</td><td></td></tr><tr><td>114</td><td>Distribution-Aware Single-Stage Models for Multi-Person 3D Pose Estimation</td><td>Zitian Wang, Xuecheng Nie, Xiaochao Qu, Yunpeng Chen, Si Liu*</td><td>2022</td><td>paper_thumb&#x2F;das.png</td><td>conference</td><td></td><td>paper&#x2F;Wang_Distribution-Aware_Single-Stage_Models_for_Multi-Person_3D_Pose_Estimation_CV_1XalJZU.pdf</td><td>CVPR 2022</td><td></td></tr><tr><td>115</td><td>Reinforced Structured State-Evolution for Vision-Language Navigation</td><td>Jinyu Chen, Chen Gao, Meng Erli,  Qiong Zhang, Si Liu*</td><td>2022</td><td>paper_thumb&#x2F;rlm.png</td><td>conference</td><td></td><td>paper&#x2F;Chen_Reinforced_Structured_State-Evolution_for_Vision-Language_Navigation_CVPR_2022_paper.pdf</td><td>CVPR 2022</td><td></td></tr><tr><td>116</td><td>3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection</td><td>Junyu Luo#, Jiahui Fu#, Xianghao Kong, Chen Gao*, Haibing Ren, Hao Shen, Huaxia Xia, Si Liu</td><td>2022</td><td>paper_thumb&#x2F;3D-SPS.png</td><td>conference</td><td></td><td>paper&#x2F;Luo_3D-SPS_Single-Stage_3D_Visual_Grounding_via_Referred_Point_Progressive_Selection.pdf</td><td>CVPR 2022 (Oral)</td><td></td></tr><tr><td>117</td><td>GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection</td><td>Yue Liao, Aixi Zhang, Miao Lu, Yongliang Wang, Xiaobo Li, Si Liu*</td><td>2022</td><td>paper_thumb&#x2F;gen-vlkt.png</td><td>conference</td><td>https:&#x2F;&#x2F;github.com&#x2F;YueLiao&#x2F;gen-vlkt</td><td>paper&#x2F;Liao_GEN-VLKT_Simplify_Association_and_Enhance_Interaction_Understanding_for_HOI_D_VnwXiy6.pdf</td><td>CVPR 2022</td><td></td></tr><tr><td>118</td><td>Fine-grained Face Editing via Personalized Spatial-aware Affine Modulation</td><td>Si Liu, Renda Bao, Defa Zhu, Shaofei Huang, Qiong Yan, Liang Lin, Chao Dong</td><td>2022</td><td>paper_thumb&#x2F;MBGAN.png</td><td>journal</td><td></td><td>paper&#x2F;Fine-grained_Face_Editing_via_Personalized_Spatial-aware_Affine_Modulation.pdf</td><td>TMM 2022</td><td></td></tr><tr><td>119</td><td>Progressive Language-customized Visual Feature Learning for One-stage Visual Grounding</td><td>Yue Liao, Aixi Zhang, Zhiyuan Chen, Tianrui Hui, Si Liu*</td><td>2022</td><td>paper_thumb&#x2F;Snipaste_2022-05-26_19-30-22.png</td><td>journal</td><td></td><td>paper&#x2F;Progressive_Language-Customized_Visual_Feature_Learning_for_One-Stage_Visual_Grounding.pdf</td><td>TIP 2022</td><td></td></tr><tr><td>120</td><td>PPMN: Pixel-Phrase Matching Network for One-Stage Panoptic Narrative Grounding</td><td>Zihan Ding#, Zi-han Ding#, Tianrui Hui*, Junshi Huang, Xiaoming Wei, Xiaolin Wei, Si Liu</td><td>2022</td><td>paper_thumb&#x2F;PPMN.png</td><td>conference</td><td>https:&#x2F;&#x2F;github.com&#x2F;dzh19990407&#x2F;PPMN</td><td>paper&#x2F;camera_ready_v9CLiYO.pdf</td><td>ACM MM 2022</td><td></td></tr><tr><td>121</td><td>Target-Driven Structured Transformer Planner for Vision-Language Navigation</td><td>Yusheng Zhao#, Jinyu Chen#, Chen Gao, Wenguan Wang*, Lirong Yang, Haibing Ren, Huaxia Xia, Si Liu</td><td>2022</td><td>paper_thumb&#x2F;Target-Driven_Structured_Transformer_Planner_for_Vision-Language_Navigation.png</td><td>conference</td><td></td><td>paper&#x2F;2207.11201.pdf</td><td>ACM MM 2022 (Oral)</td><td></td></tr><tr><td>122</td><td>Cross-Modality Domain Adaptation for Freespace Detection: A Simple yet Effective Baseline</td><td>Yuanbin Wang, Leyan Zhu, Shaofei Huang*, Tianrui Hui, Xiaojie Li, Fei Wang, Si Liu</td><td>2022</td><td>paper_thumb&#x2F;Cross-Modality_Domain_Adaptation_for_Freespace_Detection.png</td><td>conference</td><td></td><td>paper&#x2F;33_camera_ready_v3.pdf</td><td>ACM MM 2022</td><td></td></tr><tr><td>123</td><td>PoseTrans: A Simple Yet Effective Pose Transformation Augmentation for Human Pose Estimation</td><td>Wentao Jiang, Sheng Jin, Wentao Liu, Chen Qian, Ping Luo, Si Liu*</td><td>2022</td><td>paper_thumb&#x2F;WechatIMG217.png</td><td>conference</td><td></td><td>paper&#x2F;2033.pdf</td><td>ECCV 2022</td><td></td></tr><tr><td>124</td><td>HEAD: HEtero-Assists Distillation for Heterogeneous Object Detectors</td><td>Luting Wang, Xiaojie Li, Yue Liao*, Zeren Jiang, Jianlong Wu, Fei Wang, Chen Qian, Si Liu</td><td>2022</td><td>paper_thumb&#x2F;Snipaste_2022-09-24_22-54-26.png</td><td>conference</td><td></td><td>paper&#x2F;2207.05345.pdf</td><td>ECCV 2022</td><td></td></tr><tr><td>125</td><td>Simultaneously Training and Compressing Vision-and-Language Pre-training Model</td><td>Qiaosong Qi , Aixi Zhang , Yue Liao*, wenyu sun , yongliang wang , xiaobo li , Si Liu</td><td>2022</td><td>paper_thumb&#x2F;asdf.png</td><td>journal</td><td></td><td>paper&#x2F;FINAL_VERSION_RBDxLE2.PDF</td><td>TMM 2022</td><td></td></tr><tr><td>126</td><td>Language-Aware Spatial-Temporal Collaboration for Referring Video Segmentation</td><td>Tianrui Hui, Si Liu*, Zihan Ding, Shaofei Huang, Guanbin Li, Wenguan Wang, Luoqi Liu, Jizhong Han</td><td>2023</td><td>paper_thumb&#x2F;WechatIMG36.png</td><td>journal</td><td></td><td>paper&#x2F;CSTM_TPAMI_Paper.pdf</td><td>TPAMI 2023</td><td></td></tr><tr><td>127</td><td>Analyzing Infrastructure LiDAR Placement with Realistic LiDAR Simulation Library</td><td>Xinyu Cai, Wentao Jiang, Runsheng Xu, Wenquan Zhao, Jiaqi Ma, Si Liu, Yikang Li*</td><td>2023</td><td>paper_thumb&#x2F;rsu.png</td><td>conference</td><td></td><td>paper&#x2F;2211.15975.pdf</td><td>ICRA 2023</td><td></td></tr><tr><td>128</td><td>Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection</td><td>Shaofei Huang, Zhenwei Shen, Zehao Huang, Zihan Ding, Jiao Dai, Jizhong Han, Naiyan Wang, Si Liu</td><td>2023</td><td>paper_thumb&#x2F;640.png</td><td>conference</td><td></td><td>paper&#x2F;huangshaofeii.pdf</td><td>CVPR 2023</td><td></td></tr><tr><td>129</td><td>Bridging Search Region Interaction with Template for RGB-T Tracking</td><td>Tianrui Hui, Zizheng Xun, Fengguang Peng, Junshi Huang, Xiaoming Wei, Xiaolin Wei, Jiao Dai, Jizhong Han, Si Liu</td><td>2023</td><td>paper_thumb&#x2F;641.png</td><td>conference</td><td></td><td>paper&#x2F;Hui_Bridging_Search_Region_Interaction_With_Template_for_RGB-T_Tracking_CVPR_2023_paper.pdf</td><td>CVPR 2023</td><td></td></tr><tr><td>130</td><td>DETR with Additional Global Aggregation for Cross-domain Weakly Supervised Object Detection</td><td>Zongheng Tang, Yifan Sun, Si Liu, Yi Yang</td><td>2023</td><td>paper_thumb&#x2F;642.png</td><td>conference</td><td></td><td>paper&#x2F;tzh.pdf</td><td>CVPR 2023</td><td></td></tr><tr><td>131</td><td>Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection</td><td>Luting Wang, Yi Liu, Penghui Du, Zihan Ding, Yue Liao, Qiaosong Qi, Biaolong Chen, Si Liu</td><td>2023</td><td>paper_thumb&#x2F;643.png</td><td>conference</td><td></td><td>paper&#x2F;2303.05892.pdf</td><td>CVPR 2023</td><td></td></tr><tr><td>132</td><td>Adaptive Zone-aware Hierarchical Planner for Vision-Language Navigation</td><td>Chen Gao, Xingyu Peng, Bo Yan, He Wang, Lirong Yang, Haibing Ren, Hongsheng Li, Si Liu</td><td>2023</td><td>paper_thumb&#x2F;644.png</td><td>conference</td><td></td><td>paper&#x2F;Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2_DUbwyiJ.pdf</td><td>CVPR 2023</td><td></td></tr><tr><td>133</td><td>Improving Weakly Supervised Temporal Action Localization by Bridging Train-Test Gap in Pseudo Labels</td><td>Jingqiu Zhou, Liang Wang, Si Liu, Hongsheng Li, Linjiang Huang</td><td>2023</td><td>paper_thumb&#x2F;645.png</td><td>conference</td><td></td><td>paper&#x2F;2304.07978.pdf</td><td>CVPR 2023</td><td></td></tr><tr><td>134</td><td>LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT</td><td>Le Zhuo, Ruibin Yuan, Jiahao Pan, Yinghao Ma, Yizhi LI, Ge Zhang, Si Liu, Roger Dannenberg, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wenhu Chen, Wei Xue, Yike Guo</td><td>2023</td><td>paper_thumb&#x2F;WechatIMG577.jpeg</td><td>conference</td><td></td><td>paper&#x2F;438_lyricwhiz_robust_multilingual_.pdf</td><td>ISMIR 2023</td><td></td></tr><tr><td>135</td><td>Sparse Dense Fusion for 3D Object Detection</td><td>Yulu Gao, Chonghao Sima, Shaoshuai Shi, Shangzhe Di, Si Liu, Hongyang Li</td><td>2023</td><td>paper_thumb&#x2F;WechatIMG581.jpeg</td><td>conference</td><td></td><td>paper&#x2F;2304.04179.pdf</td><td>IROS 2023</td><td></td></tr><tr><td>136</td><td>Discovering Sounding Objects by Audio Queries for Audio Visual Segmentation</td><td>Shaofei Huang, Han Li, Yuqing Wang, Hongji Zhu, Jiao Dai, Jizhong Han, Wenge Rong and Si Liu</td><td>2023</td><td>paper_thumb&#x2F;WechatIMG583.jpeg</td><td>conference</td><td></td><td>paper&#x2F;Discovering_Sounding_Objects_by_Audio_Queries_for_Audio_Visual_Segmentation.pdf</td><td>IJCAI 2023</td><td></td></tr><tr><td>137</td><td>Enriching Phrases with Coupled Pixel and Object Contexts for Panoptic Narrative Grounding</td><td>Tianrui Hui，Zihan Ding，Junshi Huang，Xiaoming Wei，Xiaolin Wei，Jiao Dai，Jizhong Han，Si Liu</td><td>2023</td><td>paper_thumb&#x2F;WechatIMG585.jpeg</td><td>conference</td><td></td><td>paper&#x2F;Enriching_Phrases_with_Coupled_Pixel_and_Object_Contexts_for_Panoptic_Narrative_Grounding.pdf</td><td>IJCAI 2023</td><td></td></tr><tr><td>138</td><td>Video Background Music Generation: Dataset, Method and Evaluation</td><td>Le Zhuo, Zhaokai Wang, Baisen Wang, Yue Liao, Stanley Peng, Chenxi Bao, Miao Lu, Xiaobo Li, Si Liu</td><td>2023</td><td>paper_thumb&#x2F;Video_Background.png</td><td>conference</td><td></td><td>paper&#x2F;Video_Background_Music_Generation_Dataset_Method_and_Evaluation.pdf</td><td>ICCV 2023</td><td></td></tr><tr><td>139</td><td>Optimizing the Placement of Roadside LiDARs for Autonomous Driving</td><td>Wentao Jiang, Hao Xiang, Xinyu Cai, Runsheng Xu, Jiaqi Ma, Yikang Li, Gim Hee Lee, Si Liu</td><td>2023</td><td>paper_thumb&#x2F;Optimizing_the_Placement_of_Roadside_LiDARs_for_Autonomous_Driving.png</td><td>conference</td><td></td><td>paper&#x2F;Optimizing_the_Placement_of_Roadside_LiDARs_for_Autonomous_Driving.pdf</td><td>ICCV 2023</td><td></td></tr><tr><td>140</td><td>Omnidirectional Information Gathering for Knowledge Transfer-based Audio-Visual Navigation</td><td>Jinyu Chen, Wenguan Wang, Si Liu, Hongsheng Li, Yi Yang</td><td>2023</td><td>paper_thumb&#x2F;Omnidirectional_Information_Gathering_for_Knowledge_Transfer-based_Audio-Vis_LFmHgTi.png</td><td>conference</td><td></td><td>paper&#x2F;Omnidirectional_Information_Gathering_for_Knowledge_Transfer-based_Audio-Visual_Navigation.pdf</td><td>ICCV 2023</td><td></td></tr><tr><td>141</td><td>Object as Query: Lifting any 2D Object Detector to 3D Detection</td><td>Zitian Wang, Zehao Huang, Jiahui Fu, Naiyan Wang, Si Liu</td><td>2023</td><td>paper_thumb&#x2F;Object_as_Query-_Lifting_any_2D_Object_Detector_to_3D_Detection.png</td><td>conference</td><td></td><td>paper&#x2F;Object_as_Query-_Lifting_any_2D_Object_Detector_to_3D_Detection_.pdf</td><td>ICCV 2023</td><td></td></tr><tr><td>142</td><td>DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation</td><td>Qiaosong Qi*, Le Zhuo*, Aixi Zhang, Yue Liao, Fei Fang, Si Liu, Shuicheng Yan</td><td>2023</td><td>paper_thumb&#x2F;Cascaded_Human_Motion_Diffusion_Model_for_Dance_Generation.png</td><td>conference</td><td></td><td>paper&#x2F;DiffDance.pdf</td><td>ACM MM 2023</td><td></td></tr><tr><td>143</td><td>Transferring CLIP’s Knowledge into Zero-Shot Point Cloud Semantic Segmentation</td><td>Yuanbin Wang, Shaofei Huang, Yulu Gao, Zhen Wang, Rui Wang, Kehua Sheng, Bo Zhang, Si Liu</td><td>2023</td><td>paper_thumb&#x2F;Transferring_CLIPs_Knowledge_into_Zero-Shot_Point_Cloud_Semantic_Segmentation.png</td><td>conference</td><td></td><td>paper&#x2F;Transferring_CLIPs_Knowledge_into_Zero-Shot_Point_Cloud_Semantic_Segmentation.pdf</td><td>ACM MM 2023</td><td></td></tr><tr><td>144</td><td>DUSA: Decoupled Unsupervised Sim2Real Adaptation for Vehicle-to-Everything Collaborative Perception</td><td>Xianghao Kong, Wentao Jiang, Jinrang Jia, Yifeng Shi, Runsheng Xu, Si Liu</td><td>2023</td><td>paper_thumb&#x2F;Decoupled_Unsupervised_Sim2Real_Adaptation_for_Vehicle-to-Everything_Collabo_yGvqoKj.png</td><td>conference</td><td></td><td>paper&#x2F;Sim2Real_MM_2023_Camera_Ready_2.pdf</td><td>ACM MM 2023</td><td></td></tr><tr><td>145</td><td>MARBLE: Music Audio Representation Benchmark for Universal Evaluation</td><td>Ruibin Yuan, Yinghao Ma, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Le Zhuo, Yiqi Liu, Jiawen Huang, Zeyue Tian, Binyue Deng, Ningzhi Wang, Wenhu Chen, Gus Xia, Wei Xue, Si Liu, Shi Wang, Ruibo Liu, Yike Guo, Jie Fu</td><td>2023</td><td>paper_thumb&#x2F;MARBLE.png</td><td>conference</td><td></td><td>paper&#x2F;Music_Audio_Representation_Benchmark_for_Universal_Evaluation.pdf</td><td>NeurIPS 2023</td><td></td></tr><tr><td>146</td><td>Room-Object Entity Prompting and Reasoning for Embodied Referring Expression</td><td>Chen Gao, Si Liu*, Jinyu Chen, Luting Wang, Qi Wu, Bo Li, Qi Tian</td><td>2023</td><td>paper_thumb&#x2F;Room-obj.png</td><td>journal</td><td></td><td>paper&#x2F;Room-Object_Entity_Prompting_and_Reasoning_for_Embodied_Referring_Expression.pdf</td><td>TPAMI 2023</td><td></td></tr><tr><td>147</td><td>Region-Adaptive and Context-Complementary Cross Modulation for RGB-T Semantic Segmentation</td><td>Fengguang Peng, Zihan Ding, Ziming Chen , Gang Wang*, Tianrui Hui, Si Liu, Hang Shi</td><td>2023</td><td>paper_thumb&#x2F;WeChat8d63533030f890e74b38cdf0b10cfae5.png</td><td>journal</td><td></td><td>paper&#x2F;PR_pfg.pdf</td><td>Pattern Recognition 2023</td><td></td></tr><tr><td>148</td><td>Delving into the Devils of Bird’s-eye-view Perception: A Review, Evaluation and Recipe</td><td>Hongyang Li*, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu*, Huijie Wang, Jia Zeng, Zhiqi Li, Jiazhi Yang, Hanming Deng, Hao Tian, Enze Xie, Jiangwei Xie, Li Chen, Tianyu Li, Yang Li, Yulu Gao, Xiaosong Jia, Si Liu, Jianping Shi, Dahua Lin, Yu Qiao</td><td>2023</td><td>paper_thumb&#x2F;delving.png</td><td>journal</td><td></td><td>paper&#x2F;Delving.pdf</td><td>TPAMI 2023</td><td></td></tr><tr><td>149</td><td>MI3C: Mining Intra- and Inter-Image Context for Person Search</td><td>Zongheng Tang, Yulu Gao, Tianrui Hui*, Fengguang Peng, Si Liu</td><td>2023</td><td>paper_thumb&#x2F;MI3C.png</td><td>journal</td><td></td><td>paper&#x2F;MI3C.pdf</td><td>Pattern Recognition 2023</td><td></td></tr><tr><td>150</td><td>Linker: Learning Long Short-term Associations for Robust Visual Tracking</td><td>Zizheng Xun, Shangzhe Di, Yulu Gao, Zongheng Tang, Gang Wang∗, Si Liu, Bo Li</td><td>2023</td><td>paper_thumb&#x2F;link.png</td><td>journal</td><td></td><td>paper&#x2F;Linker_draft.pdf</td><td>TMM 2023</td><td></td></tr><tr><td>151</td><td>Teach-DETR: Better Training DETR with Teachers</td><td>Linjiang Huang, Kaixin Lu, Guanglu Song, Liang Wang, Si Liu, Yu Liu, Hongsheng Li*</td><td>2023</td><td>paper_thumb&#x2F;Teach-DETR.png</td><td>journal</td><td></td><td>paper&#x2F;Teach-DETR_Better_Training_DETR_With_Teachers.pdf</td><td>TPAMI 2023</td><td></td></tr><tr><td>152</td><td>ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation</td><td>Bo Zhang#, Xinyu Cai#*, Jiakang Yuan, Donglin Yang, Jianfei Guo, Xiangchao Yan, Renqiu Xia, Botian Shi, Min Dou, Tao Chen, Si Liu, Junchi Yan*, Yu Qiao</td><td>2024</td><td>paper_thumb&#x2F;ReSimAD.png</td><td>conference</td><td></td><td>paper&#x2F;ReSimAD.pdf</td><td>ICLR 2024</td><td></td></tr><tr><td>153</td><td>Octavius: Mitigating Task Interference in MLLMs via MoE</td><td>Ziqin Wang#, Zeren Chen#, Zhen Wang#, Huayang Liu, Zhenfei Yin, Si Liu, Lu Sheng*, Wanli Ouyang, Yu Qiao, Jing Shao*</td><td>2024</td><td>paper_thumb&#x2F;OCTAVIUS.png</td><td>conference</td><td></td><td>paper&#x2F;Octavius.pdf</td><td>ICLR 2024</td><td></td></tr><tr><td>154</td><td>Eliminating Cross-modal Conflicts in BEV Space for LiDAR-Camera 3D Object Detection</td><td>Jiahui Fu, Chen Gao, Zitian Wang, Lirong Yang, Xiaofei Wang, Beipeng Mu, Si Liu</td><td>2024</td><td>paper_thumb&#x2F;Eliminating_Cross-modal_Conflicts_in_BEV_Space_.png</td><td>conference</td><td></td><td>paper&#x2F;Eliminating_Cross-modal_Conflicts_in_BEV_Space_for_LiDAR-Camera_3D_Object_Detection.pdf</td><td>ICRA 2024</td><td></td></tr><tr><td>155</td><td>Multi-Person Pose Regression with Distribution-Aware Single-Stage Models</td><td>Leyan Zhu#, Zitian Wang#, Si Liu*, Xuecheng Nie, Luoqi Liu, Bo Li</td><td>2024</td><td>paper_thumb&#x2F;DAS_0VQGEse.png</td><td>journal</td><td></td><td>paper&#x2F;TPAMI_23_1_final.pdf</td><td>TPAMI 2024</td><td></td></tr><tr><td>156</td><td>Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training</td><td>Runze He#, Shaofei Huang#, Xuecheng Nie, Tianrui Hui, Luoqi Liu, Jiao Dai, Jizhong Han, Guanbin Li*, Si Liu*</td><td>2024</td><td>paper_thumb&#x2F;Customize_your_NeRF.png</td><td>conference</td><td></td><td>paper&#x2F;Customize_your_NeRF.pdf</td><td>CVPR 2024</td><td></td></tr><tr><td>157</td><td>EASE-DETR: Easing the Competition among Object Queries</td><td>Yulu Gao, Yifan Sun, Xudong Ding, Chuyang Zhao, Si Liu</td><td>2024</td><td>paper_thumb&#x2F;CVPR_2024_Gao.jpg</td><td>conference</td><td></td><td>paper&#x2F;Easing_the_Competition_among_Object_Queries.pdf</td><td>CVPR 2024</td><td></td></tr><tr><td>158</td><td>Data Augmentation in Human-Centric Vision</td><td>Wentao Jiang, Yige Zhang, Shaozhong Zheng, Si Liu*, Shuicheng Yan</td><td>2024</td><td>paper_thumb&#x2F;human_centirc_vision.png</td><td>journal</td><td></td><td>paper&#x2F;Data_Augmentation_in_Human-Centric_Vision.pdf</td><td>Vicinagearth (Springer Nature) 2024</td><td></td></tr><tr><td>159</td><td>Reference Prompted Model Adaptation for Referring Camouflaged Object Detection</td><td>Xuewei Liu#, Shaofei Huang#, Ruipu Wu, Hengyuan Zhao, Duo Xu, Xiaoming Wei, Jizhong Han*, Si Liu</td><td>2024</td><td>paper_thumb&#x2F;Reference_Prompted_Model_Adaptation.png</td><td>conference</td><td></td><td>paper&#x2F;Reference_Prompted_Model_Adaptation_for_Referring_Camouflaged_Object_Detection.pdf</td><td>ICME 2024</td><td></td></tr><tr><td>160</td><td>FeatAug-DETR: Enriching One-to-Many Matching for DETRs with Feature Augmentation</td><td>Rongyao Fang, Peng Gao, Aojun Zhou, Yingjie Cai, Si Liu, Jifeng Dai,  Hongsheng Li*</td><td>2024</td><td>paper_thumb&#x2F;FeatAug-DETR.png</td><td>journal</td><td></td><td>paper&#x2F;FeatAug-DETR.pdf</td><td>TPAMI 2024</td><td></td></tr><tr><td>161</td><td>PPDM++: Parallel Point Detection and Matching for Fast and Accurate HOI Detection</td><td>Yue Liao, Si Liu*, Yulu Gao, Aixi Zhang, Zhimin Li, Fei Wang, Bo Li</td><td>2024</td><td>paper_thumb&#x2F;PPDM.png</td><td>journal</td><td></td><td>paper&#x2F;PAMI_PPDM_final.pdf</td><td>TPAMI 2024</td><td></td></tr><tr><td>162</td><td>SAFDNet: A Simple and Effective Network for Fully Sparse 3D Object Detection</td><td>Gang Zhang, Junnan Chen, Guohuan Gao, Jianmin Li, Si Liu, Xiaolin Hu*</td><td>2024</td><td>paper_thumb&#x2F;SAFDnet.png</td><td>conference</td><td></td><td>paper&#x2F;SAFDNet.pdf</td><td>CVPR 2024</td><td></td></tr><tr><td>163</td><td>Learning Background Prompts to Discover Implicit Knowledge for Open Vocabulary Object Detection</td><td>Jiaming Li, Jiacheng Zhang, Jichang Li, Ge Li, Si Liu, Liang Lin, Guanbin Li</td><td>2024</td><td>paper_thumb&#x2F;Leaning_Background.png</td><td>conference</td><td></td><td>paper&#x2F;Learning_Background_Prompts_to_Discover_Implicit_Knowledge_for_Open_Vocabulary.pdf</td><td>CVPR 2024</td><td></td></tr><tr><td>164</td><td>Communication-Efficient Collaborative Perception via Information Filling with Codebook</td><td>Yue Hu, Juntong Peng, Sifei Liu, Junhao Ge, Si Liu, Siheng Chen</td><td>2024</td><td>paper_thumb&#x2F;comm_effient.png</td><td>conference</td><td></td><td>paper&#x2F;Communication-Efficient_Collaborative_Perception.pdf</td><td>CVPR 2024</td><td></td></tr><tr><td>165</td><td>MAC: Masked Contrastive Pre-Training for Efficient Video-Text Retrieval</td><td>Fangxun Shu, Biaolong Chen, Yue Liao, Jinqiao Wang, Si Liu</td><td>2024</td><td>paper_thumb&#x2F;SCR-20240517-npdo.png</td><td>journal</td><td></td><td>paper&#x2F;MAC__Masked_Contrastive_Pre_Training_for_Efficient_Video_Text_Retrieval_1.pdf</td><td>TMM 2024</td><td></td></tr><tr><td>166</td><td>Mask-Enhanced Segment Anything Model for Tumor Lesion Semantic Segmentation</td><td>Hairong Shi#, Songhao Han#, Shaofei Huang*, Yue Liao, Guanbin Li*, Xiangxing Kong, Hua Zhu, Xiaomu Wang, Si Liu</td><td>2024</td><td>paper_thumb&#x2F;MICCAI.png</td><td>conference</td><td></td><td>paper&#x2F;MICCAI__Camera_Ready_.pdf</td><td>MICCAI 2024</td><td></td></tr><tr><td>167</td><td>Realistic Rainy Weather Simulation for LiDARs in CARLA Simulator</td><td>Donglin Yang, Xinyu Cai∗, Zhenfeng Liu, Wentao Jiang, Bo Zhang, Guohang Yan, Xing Gao, Si Liu, Botian Shi</td><td>2024</td><td>paper_thumb&#x2F;SCR-20240701-nsix.png</td><td>conference</td><td></td><td>paper&#x2F;Realistic_Rainy_Weather_Simulation_for_LiDARs_in_CARLA_Simulator.pdf</td><td>IROS 2024</td><td></td></tr><tr><td>168</td><td>Asynchronous Large Language Model Enhanced Planner for Autonomous Driving</td><td>Yuan Chen#, Zi-han Ding#, Ziqin Wang#, Yan Wang*, Lijun Zhang, Si Liu*</td><td>2024</td><td>paper_thumb&#x2F;SCR-20240702-osbm.png</td><td>conference</td><td></td><td>paper&#x2F;Asynchronous_Large_Language_Model_Enhanced_Planner.pdf</td><td>ECCV 2024</td><td></td></tr><tr><td>169</td><td>Global-Local Collaborative Inference with LLM for Lidar-Based Open-Vocabulary Detection</td><td>Peng, Yan Bai, Chen Gao, Lirong Yang, Fei Xia, Beipeng Mu, Xiaofei Wang, Si Liu</td><td>2024</td><td>paper_thumb&#x2F;xingyu.jpg</td><td>conference</td><td></td><td>paper&#x2F;Global-Local_Collaborative_Inference.pdf</td><td>ECCV 2024</td><td></td></tr><tr><td>170</td><td>LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction</td><td>Penghui Du#, Yu Wang#, Yifan Sun, Luting Wang, Yue Liao, Gang Zhang, Errui Ding, Yan Wang*, Jingdong Wang, Si Liu*</td><td>2024</td><td>paper_thumb&#x2F;penghui.png</td><td>conference</td><td></td><td>paper&#x2F;penghui_zI1agBH.pdf</td><td>ECCV 2024</td><td></td></tr><tr><td>171</td><td>Controllable Navigation Instruction Generation with Chain of Thought Prompting</td><td>Xianghao Kong#, Jinyu Chen#, Wenguan Wang*, Hang Su, Xiaolin Hu, Yi Yang, Si Liu*</td><td>2024</td><td>paper_thumb&#x2F;xianghao.jpg</td><td>conference</td><td></td><td>paper&#x2F;VLN_ECCV_2024.pdf</td><td>ECCV 2024</td><td></td></tr><tr><td>172</td><td>FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis</td><td>Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, Hongsheng Li</td><td>2024</td><td>paper_thumb&#x2F;linjiang.png</td><td>conference</td><td></td><td>paper&#x2F;FouriScale.pdf</td><td>ECCV 2024</td><td></td></tr><tr><td>173</td><td>GPD-VVTO: Preserving Garment Details in Video Virtual Try-On</td><td>Yuanbin Wang, Weilun Dai, Long Chan, Huanyu Zhou, Aixi Zhang, Si Liu</td><td>2024</td><td>paper_thumb&#x2F;yuanbin.jpg</td><td>conference</td><td></td><td>paper&#x2F;3664647.3680701.pdf</td><td>ACM MM 2024</td><td></td></tr><tr><td>174</td><td>Collaborative Training of Tiny-Large Vision Language Models</td><td>Shichen Lu, Longteng Guo, Wenxuan Wang, Zijia Zhao, Tongtian Yue,  Si Liu, Jing Liu</td><td>2024</td><td>paper_thumb&#x2F;20240716214216.jpg</td><td>conference</td><td></td><td>paper&#x2F;2264_Collaborative_Training_of.pdf</td><td>ACM MM 2024</td><td></td></tr><tr><td>175</td><td>Dynamic Prompting of Frozen Text-to-Image Diffusion Models for Panoptic Narrative Grounding</td><td>Hongyu Li, Tianrui Hui, Zihan Ding, Jing Zhang, Bin MA, Xiaoming Wei, Jizhong Han, Si Liu</td><td>2024</td><td>paper_thumb&#x2F;WechatIMG188.jpg</td><td>conference</td><td></td><td>paper&#x2F;Dynamic_Prompting_of_Frozen_Text-to-Image_Diffusion_Models_for_Panopic_Narrative_Grounding.pdf</td><td>ACM MM 2024</td><td></td></tr><tr><td>176</td><td>Lumina-Next: Making Lumina-T2X Stronger and Faster with Next-DiT</td><td>Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, Xu Luo, Zehan Wang, Kaipeng Zhang, Xiangyang Zhu, Si Liu, Xiangyu Yue, Dingning Liu, Wanli Ouyang, Ziwei Liu, Yu Qiao, Hongsheng Li, Peng</td><td>2024</td><td>paper_thumb&#x2F;zhuole.png</td><td>conference</td><td></td><td>paper&#x2F;2406.18583v1.pdf</td><td>NeurIPS 2024</td><td></td></tr><tr><td>177</td><td>Image Understanding Makes for A Good Tokenizer for Image Generation</td><td>Luting Wang, Yang Zhao, Zijian Zhang, Jiashi Feng, Si Liu, Bingyi Kang</td><td>2024</td><td>paper_thumb&#x2F;Luting.png</td><td>conference</td><td></td><td>paper&#x2F;2411.04406v1.pdf</td><td>NeurIPS 2024</td><td></td></tr><tr><td>178</td><td>CooHOI: Learning Cooperative Human-Object Interaction with Manipulated Object Dynamics</td><td>Ziqin Wang*, Jiawei Gao*, Zeqi Xiao, Jingbo Wang, Tai Wang, Jinkun Cao, Xiaolin Hu, Si Liu, Jifeng Dai, Jiangmiao Pang</td><td>2024</td><td>paper_thumb&#x2F;ziqin.png</td><td>conference</td><td></td><td>paper&#x2F;CooHOI_Learning_Cooperative_Human-Object.pdf</td><td>NeurIPS 2024 (Spotlight)</td><td></td></tr><tr><td>179</td><td>RGB-T Tracking with Template-Bridged Search Interaction and Target-Preserved Template Updating</td><td>Bo Li, Fengguang Peng, Tianrui Hui, Xiaoming Wei, Xiaolin Wei, Lijun Zhang, Hang Shi, Si Liu*</td><td>2024</td><td>paper_thumb&#x2F;RGB-T_Tracking.png</td><td>journal</td><td></td><td>paper&#x2F;TPAMI3475472.pdf</td><td>TPAMI 2024</td><td></td></tr><tr><td>180</td><td>Anchor3DLane++: 3D Lane Detection via Sample-Adaptive Sparse 3D Anchor Regression</td><td>Shaofei Huang, Zhenwei Shen, Zehao Huang, Yue Liao, Jizhong Han, Naiyan Wang, Si Liu*</td><td>2024</td><td>paper_thumb&#x2F;WechatIMG459.jpg</td><td>journal</td><td></td><td>paper&#x2F;Anchor3DLane.pdf</td><td>TPAMI 2024</td><td></td></tr><tr><td>181</td><td>GaussianPainter: Painting Point Cloud into 3D Gaussians with Normal Guidance</td><td>Jingqiu Zhou*, Lue Fan*, Xuesong Chen, Linjiang Huang#, Si Liu, Hongsheng Li</td><td>2025</td><td>paper_thumb&#x2F;bb8c3d030bfe96f8079f368e65b4d75c.png</td><td>conference</td><td></td><td>paper&#x2F;2412.17715v1.pdf</td><td>AAAI 2025</td><td></td></tr><tr><td>182</td><td>Unleashing the Temporal-Spatial Reasoning Capacity of GPT for Training-Free Audio and Language Referenced Video Object Segmentation</td><td>Shaofei Huang*, Rui Ling*, Hongyu Li*, Tianrui Hui, Zongheng Tang, Xiaoming Wei, Jizhong Han, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;607c45fca634c6ca8ff3c8dbf215ba45.png</td><td>conference</td><td></td><td>paper&#x2F;2408.15876v1.pdf</td><td>AAAI 2025</td><td></td></tr><tr><td>183</td><td>Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology</td><td>Xiangyu Wang*, Donglin Yang*, Ziqin Wang*, Hohin Kwan, Jinyu Chen, Wenjun Wu, Hongsheng Li, Yue Liao#, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;xiangyu.png</td><td>conference</td><td></td><td>paper&#x2F;2410.07087v2.pdf</td><td>ICLR 2025</td><td></td></tr><tr><td>184</td><td>LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation</td><td>Fangxun Shu*, Yue Liao*, Le Zhuo, Chenning Xu, Guanghao Zhang, Haonan Shi, Long Chen, Tao Zhong, Wanggui He, Siming Fu, Haoyuan Li, Bolin Li, Zhelun Yu, Si Liu#, Hongsheng Li#, Hao Jiang#</td><td>2025</td><td>paper_thumb&#x2F;Liaoyue2.png</td><td>conference</td><td></td><td>paper&#x2F;2408.15881v3.pdf</td><td>ICLR 2025</td><td></td></tr><tr><td>185</td><td>Point Cluster: A Compact Message Unit for Communication-Efficient Collaborative Perception</td><td>Zihan Ding, Jiahui Fu, Si Liu#, Hongyu Li, Siheng Chen, Hongsheng Li, Shifeng Zhang, Xu Zhou</td><td>2025</td><td>paper_thumb&#x2F;dingzihan.png</td><td>conference</td><td></td><td>paper&#x2F;13011_Point_Cluster_A_Compact_.pdf</td><td>ICLR 2025</td><td></td></tr><tr><td>186</td><td>MC-MoE: Mixture Compressor for Mixture-of-Experts LLMs Gains More</td><td>Wei Huang*, Yue Liao*, Jianhui Liu, Ruifei He, Haoru Tan, Shiming Zhang#, Hongsheng Li, Si Liu#, Xiaojuan Qi#</td><td>2025</td><td>paper_thumb&#x2F;liaoyue1.png</td><td>conference</td><td></td><td>paper&#x2F;MIXTURE_COMPRESSOR.pdf</td><td>ICLR 2025</td><td></td></tr><tr><td>187</td><td>Generative Map Priors for Collaborative BEV Semantic Segmentation</td><td>Jiahui Fu, Yue Gong, Luting Wang, Shifeng Zhang, Xu Zhou, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;Generative_Map_Priors_for_Collaborative_BEV_Semantic_Segmentation.png</td><td>conference</td><td></td><td>paper&#x2F;CVPR25_V2XSeg_camera_ready.pdf</td><td>CVPR 2025</td><td></td></tr><tr><td>188</td><td>FlexDrive: Toward Trajectory Flexibility in Driving Scene Reconstruction and Rendering</td><td>Jingqiu Zhou, Lue Fan, Linjiang Huang#, Zhaoxiang Zhang, Xiaoyu Shi, Si Liu, Hongsheng Li#</td><td>2025</td><td>paper_thumb&#x2F;acd483155e20837131cf2e4abcb2f343_.jpg</td><td>conference</td><td></td><td>paper&#x2F;FlexDrive.pdf</td><td>CVPR 2025</td><td></td></tr><tr><td>189</td><td>VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection</td><td>Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao#, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;VideoEspresso.jpg</td><td>conference</td><td></td><td>paper&#x2F;VideoEspresso.pdf</td><td>CVPR 2025 (Oral)</td><td></td></tr><tr><td>190</td><td>LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding</td><td>Hongyu Li*, Jinyu Chen*, Ziyu Wei*, Shaofei Huang, Tianrui Hui, Jialin Gao#, Xiaoming Wei, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;LLaVA-ST.png</td><td>conference</td><td></td><td>paper&#x2F;LLaVA_ST.pdf</td><td>CVPR 2025</td><td></td></tr><tr><td>191</td><td>Revisiting Audio-Visual Segmentation with Vision-Centric Transformer</td><td>Shaofei Huang*, Rui Ling, Tianrui Hui#, Hongyu Li, Xu Zhou, Shifeng Zhang, Si Liu#, Richang Hong, Meng Wang</td><td>2025</td><td>paper_thumb&#x2F;hsf2025cvpr.png</td><td>conference</td><td></td><td>paper&#x2F;2025075233.pdf</td><td>CVPR 2025</td><td></td></tr><tr><td>192</td><td>Instruction-Oriented Preference Alignment for Enhancing Multi-Modal Comprehension Capability of MLLMs</td><td>Zitian Wang, Yue Liao#, Kang Rong, Fengyun Rao, Yibo Yang#, Si Liu</td><td>2025</td><td>paper_thumb&#x2F;instruction.png</td><td>conference</td><td></td><td>paper&#x2F;2503.20309v1.pdf</td><td>ICCV 2025</td><td></td></tr><tr><td>193</td><td>Video2BEV: Transforming Drone Videos to BEVs for Video-based Geo-localization</td><td>Hao Ju*, Shaofei Huang*, Si Liu, Zhedong Zheng#</td><td>2025</td><td>paper_thumb&#x2F;Video2BEV.jpg</td><td>conference</td><td></td><td>paper&#x2F;2411.13610v3.pdf</td><td>ICCV 2025</td><td></td></tr><tr><td>194</td><td>CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective</td><td>Zongheng Tang, Yi Liu, Yifan Sun, Yulu Gao, Jinyu Chen, Runsheng Xu, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;COST.jpg</td><td>conference</td><td></td><td>paper&#x2F;2508.00359v1.pdf</td><td>ICCV 2025</td><td></td></tr><tr><td>195</td><td>CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation</td><td>Yi Liu, Shengqian Li, Zuzeng Lin, Feng Wang, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;cybervar.png</td><td>conference</td><td></td><td>paper&#x2F;2506.23347v2.pdf</td><td>ICCV 2025</td><td></td></tr><tr><td>196</td><td>RATopo: Improving Lane Topology Reasoning via Redundancy Assignment</td><td>Han Li, Shaofei Huang, Longfei Xu, Yulu Gao, Beipeng Mu, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;fig2_pipeline.pdf</td><td>conference</td><td></td><td>paper&#x2F;2508.15272v1.pdf</td><td>ACM MM 2025</td><td></td></tr><tr><td>197</td><td>DOMR: Establishing Cross-View Segmentation via Dense Object Matching</td><td>Jitong Liao*, Yulu Gao*, Shaofei Huang, Jialin Gao, Jie Lei, Ronghua Liang, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;pipeline-250412.pdf</td><td>conference</td><td></td><td>paper&#x2F;2508.04050v1.pdf</td><td>ACM MM 2025</td><td></td></tr><tr><td>198</td><td>AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation</td><td>Ruipu Wu*, Yige Zhang*, Jinyu Chen, Linjiang Huang#, Shifeng Zhang, Xu Zhou, Liang Wang, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;aeroduo.png</td><td>conference</td><td></td><td>paper&#x2F;2508.15232v1.pdf</td><td>ACM MM 2025</td><td></td></tr><tr><td>199</td><td>&quot;Hi AirStar, Guide Me to the Badminton Court.&quot;</td><td>Ziqin Wang*, Jinyu Chen*, Xiangyi Zheng, Qinan Liao, Linjiang Huang#, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;airstar.png</td><td>conference</td><td>https:&#x2F;&#x2F;buaa-colalab.github.io&#x2F;airstar.github.io</td><td>paper&#x2F;2507.04430v1.pdf</td><td>ACM MM demo 2025</td><td></td></tr><tr><td>200</td><td>MV2DFusion: Leveraging Modality-Specific Object Semantics for Multi-Modal 3D Detection</td><td>Zitian Wang, Zehao Huang, Yulu Gao, Naiyan Wang, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;MV2DFusion.png</td><td>journal</td><td></td><td>paper&#x2F;2408.05945v2.pdf</td><td>TPAMI 2025</td><td></td></tr><tr><td>201</td><td>UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV Imitation Learning</td><td>Xiangyu Wang*, Donglin Yang*, Yue Liao*, Wenhao Zheng, Wenjun Wu, Bin Dai, Hongsheng Li, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;UAV_Flow.png</td><td>conference</td><td>https:&#x2F;&#x2F;github.com&#x2F;buaa-colalab&#x2F;UAV-Flow</td><td>paper&#x2F;UAV_Flow.pdf</td><td>NeurIPS 2025</td><td></td></tr><tr><td>202</td><td>RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation</td><td>Songhao Han*, Boxiang Qiu*, Yue Liao*, Siyuan Huang, Chen Gao, Shuicheng Yan#, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;Robocelebra.PNG</td><td>conference</td><td></td><td>paper&#x2F;2506.06677v1-2.pdf</td><td>NeurIPS 2025</td><td></td></tr><tr><td>203</td><td>Towards Realistic Earth-Observation Constellation Scheduling: Benchmark and Methodology</td><td>Luting Wang*, Yinghao Xiang*, Hongliang Huang, Dongjun Li, Chen Gao#, Si Liu#</td><td>2025</td><td>paper_thumb&#x2F;Earth-Observation.PNG</td><td>conference</td><td></td><td>paper&#x2F;2168_Towards_Realistic_Earth_O.pdf</td><td>NeurIPS 2025</td><td></td></tr><tr><td>204</td><td>FACT: Mitigating Inconsistent Hallucinations in LLMs via Fact-Driven Alternating Code-Text Training</td><td>Xinxin You, Qixin Sun, Xien Liu, Chenwei Yan, Xiao Zhang, Chen Ning, Xiangling Fu, Si Liu, Shijin Wang, Guoping Hu, Ji Wu#</td><td>2025</td><td>paper_thumb&#x2F;MIh-TCCT.png</td><td>conference</td><td></td><td>paper&#x2F;4264_FACT_Mitigating_Inconsist_3IbhaMs.pdf</td><td>NeurIPS 2025</td><td></td></tr><tr><td>205</td><td>VaccineRAG: Boosting Multimodal Large Language Models&#039; Immunity to Harmful RAG Samples</td><td>Qixin Sun*, Hengyuan Zhao*, Ziqin Wang*, Yilin Li, Kaiyou Song, Linjiang Huang#, Xiaolin Hu, Qingpei Guo#, Si Liu</td><td>2026</td><td>paper_thumb&#x2F;VaccineRAG.png</td><td>conference</td><td></td><td>paper&#x2F;2509.04502v1.pdf</td><td>AAAI 2026</td><td></td></tr><tr><td>206</td><td>AerialVLA: A Vision-Language-Action Model for Aerial Navigation with Online Dialogue</td><td>Jinyu Chen*, Hongyu Li*, Zongheng Tang, Xiaoduo Li, Wenjun Wu, Si Liu#</td><td>2026</td><td>paper_thumb&#x2F;Aerial_VLA.png</td><td>conference</td><td></td><td>paper&#x2F;AAAI__2026_AerialVLA__camera_ready.pdf</td><td>AAAI 2026</td><td></td></tr></table>